
\documentclass[sigconf,anonymous,review]{acmart}
\usepackage{float}
\usepackage{graphicx}

%activate todo's
%\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
% deactivate todos
\newcommand{\todo}[1]{ \PackageWarning{TODO:}{#1!}}


%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[Santa Clara '19]{Santa Clara '19: ACM SIGIR International Conference on the Theory of Information Retrieval}{October 02--05, 2019}{Santa Clara, California}
\acmBooktitle{Santa Clara '19: ACM SIGIR International Conference on the Theory of Information Retrieval, October 02--05, 2019, Santa Clara, California}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title[Support Passage Retrieval]{Why does this Entity matter? Support Passage Retrieval for Entity Retrieval}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Shubham Chatterjee}
\email{sc1242@cs.unh.edu}
%\orcid{1234-5678-9012}
\affiliation{
 \institution{University of New Hampshire}
  \city{Durham}
 \state{New Hampshire}
}
\author{Laura Dietz}
\email{dietz@cs.unh.edu}
\affiliation{
 \institution{University of New Hampshire}
  \city{Durham}
 \state{New Hampshire}
}

%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Chatterjee and Dietz}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
Our goal is to complement an entity ranking with human-readable explanations of how those retrieved entities are connected to the information need. In this paper, we present a formalization of the problem and present some methods which may be employed to solve this problem. In particular, we develop some joint query-entity and passage features to rank passages for query-entity pairs. We also study the effect of entity salience in this task. The effectiveness of the indicators are studied within a supervised learning-to-rank framework. 
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%

\begin{CCSXML}
<ccs2012>
 <concept>
<concept_id>10002951.10003317.10003338</concept_id>
<concept_desc>Information systems~Retrieval models and ranking</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
<concept>
<concept_id>10002951.10003317.10003325</concept_id>
<concept_desc>Information systems~Information retrieval query processing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
\end{CCSXML}

\ccsdesc[500]{Information systems~Retrieval models and ranking}
\ccsdesc[300]{Information systems~Information retrieval query processing}



%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{joint query-entity-passage features, entity context neighbors, entity salience, pseudo-document}


%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

%\todo{please remove "data mining" and question answering -- none of them fit. And they have their own conferences.  Just pick something general like information retrieval and entities}


\section{Introduction}
\label{sec:Introduction}

Entity ranking is important for applications which seek either a particular entity or a list of entities, for example, entities involved in the \textit{Brexit} such as \texit{Theresa May}.  Several studies show that 40-70\% of all web searches target entities \cite{guo2009named,balog2018entity}. Such queries are best answered by giving the user a ranking of relevant entities. However, in addition to the ranking of entities, presenting a short passage to the user which tells them how or why the entity is related to the query is more helpful since it helps users decide if they want to know more about the entity. Analogous to how most modern search engines display snippets for the query to the user in case of document retrieval, we want to present a short passage about the entity that explains the user why the entity is relevant to the query without conducting further research. We study the following task:

\textbf{Task:} Given a user's information need $Q$; an external system predicts a ranking of relevant entities $E$. Our task is to, for every relevant entity $e_i \in E$, retrieve and rank $K$ passages $s_{ik}$ that explain why this entity $e_i$ is relevant for $Q$. 


% (you already said this; I used this instead of example above) For example, if we know that the entity \textit{Theresa May} is relevant to the query \textit{brexit}, we want a passage that is not only relevant and mentions \textit{Theresa May} in the context of the query but also that the entity is central to the passage, that is, salient in the passage. 
In addition to being relevant for $Q$, those passages $s_{ik}$ should mention the entity $e_i$ in a salient, central way.  In this paper, we look at the problem of complementing an entity ranking with descriptions of why the entity is relevant to the query and present some first results. This task is useful whenever entities are displayed along with web search results, such as entity cards \cite{berntson2012providing}. The novel contribution of this work is that we consider a joint model to rank passages for a given query and entity and compare it to two baselines, one which is a state-of-the-art which retrieves passages for the query-entity pair using a compound query (made of the query and entity terms) and another which scores passages for the query-entity pair by the number of relevant entities (for the query) in the passage. We also present features which consider the salience of the entity in the passage and use them to retrieve passages. We then study the effectiveness of these methods in a learning-to-rank \cite{liu2009learning} setting.

As an example, consider the following query-entity pair and the corresponding support passage : \\
\textbf{Query: } Unfree labour \\
\textbf{Entity: } Detention (imprisonment) \\
\textbf{Support Passage: } \\
Unfree labour is a generic or collective term for those work relations, especially in modern or early modern history, in which people are employed against their will by the threat of destitution, detention, violence (including death), lawful compulsion, or other extreme hardship to themselves or to members of their families.

A measure of entity salience should help; in this work we study if we have the necessary tools to reap benefits from entity salience. The issues our study reveals are not due to failures of the salience detection model, but because the retrieval models available today are not modelling salience, and therefore passages in the pool don't mention the entity in salient ways. 



\section{Related Work}
\label{sec:related work}
\paragraph{\textbf{Sentence Retrieval}}
Sentence Retrieval retrieves relevant sentences in response to a query, question or another sentence. Tasks such as question answering \cite{cardie2000examining}, summarization, novelty detection, topic detection and tracking \cite{stokes2001first}, and information provenance make use of a sentence retrieval module as a pre-processing step. However, none of these address the task of support sentence retrieval. The closest is perhaps the work by Cardie et al.\ \cite{cardie2000examining} which discusses sentence ranking models where the query includes a constraint on a type of entity(e.g. a location, a person). 
Blanco and Zaragoza \cite{blanco2010finding} present a model that ranks entity support sentences with learning-to-rank. Their work focuses on features based on named entity recognition (NER) in combination with term-based retrieval models. 
\paragraph{\textbf{Entity Relation Explanation}}
Voskarides et al.\ \cite{voskarides2015learning} study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. They model the task as a learning-to-rank problem with a rich set of features which include textual, entity and relationship features. Aggarwal et al.\ \cite{aggarwal2016connecting} rank all the paths between any two entities in a knowledge graph. This can help in explaining relationships between seemingly unconnected entities. 

\paragraph{\textbf{Support Passage Retrieval}}
Kadry and Dietz \cite{kadry2017open} use relation extraction using OpenIE for support passage retrieval. Their work studies how relation extraction can help in support passage retrieval and what are the limitations of the current relation extraction approaches that need to be overcome. It describes a rich set of features which can be used for the task.  

None of the works above study joint query-entity and paragraph relevance nor do they consider the effect of entity salience. We propose to look at features that model the joint relevance of a paragraph to an entity and a query, which captures the entity salience in the passage. 

\section{Background: Entity Salience}
\label{sec: entity salience}
Consider the query \textit{brexit} and the entity \textit{Theresa May} retrieved for the query. Now consider the following two passages: \\
\textbf{Passage 1: }  If Labour and the Conservatives fail to reach an agreement, MPs will face a series of votes on Brexit options, which could include another referendum. Theresa May has said her government stands ready to abide by the decision of the House  if Labour does the same.\\
\textbf{Passage 2: } British Prime Minister Theresa May offered a new Brexit plan on Tuesday, in a last-ditch effort to get her still-unpopular Brexit deal approved.

We notice that both passages mention the entity Theresa May and is about the query \textit{brexit}.
However, in Passage 1 Theresa May is not central, whereas in Passage 2 it is (the passage talks about how Theresa May is affecting brexit).  This is an example of a good support passage, one which not only mentions the entity and makes a connection between the query-entity pair, but also mentions the entity in a salient, that is, central way. 


%\todo{since you have some space, I would explain what entity salience means or does not mean. It is likely that the reviewer does not know it. (yes, shocking!) SO if you spend 1 paragraph, maybe one picture with two examples following the brexit / Theresa may will be good.}


\section{Approach}
\label{sec:approach}
%  not needed: \subsection{Overview}
%\label{subsec:overview}
Given an entity for a query, we want a passage that would explain to the user why the entity is relevant to the query. We can find relevant paragraphs and entities for a query using any standard information retrieval model such as BM25. The trick is to model the relevance of the paragraph for \textit{both} the query and the entity. We model this joint relevance using the co-occurring entities with a given entity. 
In all our methods, we assume that a ranked list of entities for a given query has been provided by an external system. We seek to embellish this ranking with passages. 

\subsection{Candidate Passage Generation}
\label{subsec:candidate passage generation}
We generate the candidate set of passages using a learning-to-rank combination of the following retrieval models. The RM1/RM3 are expanding with words in this case. Later, we also use RM1/RM3 for expanding the query with entities. 
%\todo{Make clear that your RM3/RM1 is expanding with words. Later you are expanding with entities (expanding wth entities is something I like to do, but its not usual in the community}:
\begin{itemize}
    \item BM25 (default Lucene) with RM1 (BM25 + RM1)
    \item BM25 (default Lucene) with RM3 (BM25 + RM3)
    \item Language Models with Jelinek-Mercer smoothing $(\lambda = 0.4)$, with RM1 (LM-JM + RM1)
    \item Language Models with Jelinek-Mercer smoothing $(\lambda = 0.4)$, with RM3 (LM-JM + RM3)
    \item Language Models with Dirichlet smoothing, with RM1 (LM-DS + RM1)
    \item Language Models with Dirichlet smoothing, with RM3 (LM-DS + RM3)
\end{itemize}


\subsection{Features}
\label{subsec:features}
\subsubsection{Features based on co-occurring entities}
\label{subsubsec:features:1}
We assume that a passage with a lot of query-relevant entities which co-occur frequently with a given entity is likely talking about that entity. For example, if we know that the entities \textit{UK} and \textit{David Cameron} co- occur most frequently with the entity \textit{Theresa May} for the query \textit{brexit}, and a paragraph mentions these entities, then that paragraph is likely to mention the entity \textit{Theresa May}. If the paragraph is also relevant for the query \textit{brexit}, we can assume that it says something about the relation between the query and the entity. 

To find which entities are most frequently co-occurring with a given entity, we filter all passages (retrieved for the query) mentioning the entity and "stitch" them together into $D_e$ which we call a \textit{Pseudo-Document} about the entity $e$, following an idea of Dalton et al.\ \cite{dalton2014entity}. All the entities contained in this pseudo-document co-occur with the given entity (the entity the pseudo-document is about). We can then derive a distribution over these co-occurring entities which would tell us how likely we are to see the co-occurring entity $e'$ provided we have already seen the query-entity pair $(q, e)$. We derive this distribution by finding the number of times an entity $e'$ occurs in the pseudo-document. More formally, 

\begin{equation}
\label{eq:1}
    P(e'|e,q) \propto \sum_{p \in D_e} count(e'\in p)
\end{equation}
where $p$ is a paragraph, $D_e$ is the pseudo-document about entity $e$, $e'$ is an entity co-occurring with $e$, and $count(e')$ is the number of $e'$ in $p$. 
\begin{enumerate}
    \item \textbf{Entities in the entity context neighbors:} \label{en:entity context neighbors} We score a passage in a pseudo-document about $e$ by accumulating the co-occurrence scores of entities $e'$ contained therein. 
More formally, we can define a feature value $f_{qe}(p)$ of a paragraph $p \in D_e$ for query-entity pair $(q, e)$ as
\begin{equation}
\label{eq:2}
    f_{qe}(p) = \sum_{e' \in p} P(e'|e,q)
\end{equation}
\item \textbf{Query expansion using entities from pseudo-documents:} \label{en: Query expansion using entities from pseudo-documents} Using top 20 co-occurring entities to expand the query using RM1 and RM3 on a search index that has a separate field for entities. We experiment with the same variations as given in Section \ref{subsec:candidate passage generation}. Each variation becomes a feature.
\end{enumerate}
\subsubsection{Features based on pseudo-documents}
\label{subsubsec:feature:2}
\begin{enumerate}
    \item \textbf{Retrieval score of a pseudo-document} \label{en:Retrieval score of a pseudo-document}For every query, we retrieve pseudo-documents using BM25. As in Section \ref{subsubsec:features:1} (\ref{en:entity context neighbors}), we obtain a passage score by accumulating the scores of the pseudo-documents it appears in. 

    \item \textbf{Query expansion using words from pseudo-document} \label{en:Query expansion using words from pseudo-document} Similar to Section \ref{subsubsec:features:1} (\ref{en: Query expansion using entities from pseudo-documents}), we expand the query using terms from the pseudo-documents using pseudo relevance feedback. We use top 50 terms for expansion and 100 documents as the feedback set. We experiment with the same variations as given in Section \ref{subsec:candidate passage generation}. Each variation becomes a feature.
\end{enumerate}

\subsubsection{Features based on entity salience}
\label{subsubsec:feature:3}
Passages should explain why the entity is relevant to the query \textit{and} in the context of the query. For example, if we know that the entity \textit{Theresa May} is relevant to the query \textit{brexit} but we find a passage which mentions this entity in the context of agriculture in UK, then this passage does not really explain why the entity is relevant to the query. Hence, we look at entity salience as a feature to find good passages which would address our task. We use SWAT \cite{ponza2018swat} to annotate passages with salient entities and salience score and class (whether salient or not). We assign a score to a passage as follows:
\begin{equation}
    f_{qe}(p) =
\left\{
	\begin{array}{ll}
		Salience(e,p)  & \mbox{if } e \mbox{ is salient in } p \\
		0 & \mbox{otherwise}
	\end{array}
\right.
\end{equation}
where $Salience(e,p)$ is the salience score of the entity for the passage as returned by SWAT.
We use this feature in two ways:
\begin{enumerate}
    \item \textbf{Salience score of passage with co-occurring entities}.
    We re-rank passages obtained using method in Section \ref{subsubsec:features:1}(\ref{en:entity context neighbors}) using this feature.
    
    \item \textbf{Salience score of passage in a pseudo-document} We score passages in  a pseudo-document about an entity using the feature. 
\end{enumerate}
\section{Evaluation}
\label{sec:Evaluation}
\subsection{Research Questions}
\label{subsec:research questions}
Our goal is to study the following research questions.\\
\textbf{RQ1} To what extent do contextual entities affect the support passage retrieval?  \\
\textbf{RQ2} To what extent does entity salience affect the support passage retrieval?  \\
\textbf{RQ3} Are contextual entities more informative than contextual words? 

\subsection{Evaluation Paradigm}
\label{subsec:Evaluation Paradigm}
We use the dataset from the TREC Complex Answer Retrieval track \cite{dietztrec} to evaluate our methods. We use the data from \textit{benchmarkY1-train}
\footnote{\url{http://trec-car.cs.unh.edu}} for training our methods, for k-fold cross validation and as ground truth for evaluation. It includes outlines, articles and ground truth. 
The \textit{paragraphCorpus}, which consists of passages from Wikipedia, is used as the corpus. We use the entity links provided in the collection as well as those annotated using TagMe \cite{ferragina2010tagme}. We derive a ground truth for entity support passage retrieval from the ground truth of relevant passages and entities provided with the data set (article-level) as follows: any true passage that contains an entity link to the query entity is defined as relevant for the given query and entity. We apply our methods to produce a passage ranking for every query-entity pair and do 5-fold cross validation. We use RankLib \footnote{Dang, V. "The Lemur Project-Wiki-RankLib." Lemur Project,[Online]. Available: \url{http://sourceforge. net/p/lemur/wiki/RankLib}.} with coordinate ascent optimization to optimize for Mean Average Precision(MAP). We use MAP, Mean Reciprocal Rank (MRR), Precision at R (P@R) and Precision at 1 (P@1) as our evaluation metrics.  
\subsection{Baseline}
\label{subsec:Baseline}
\subsubsection{Rank score proportional to the frequency of entity links to an entity} 
\label{subsubsec: baseline 1}
We rank passages for a query-entity pair by the number of relevant entities in the passage. For example, if a passage $p$ contains entities $\{e_1,e_2\}$ and the entities $\{e_1,e_2,e_3,e_4\}$ have been retrieved for the query $q$, then the score of $p$ for each of the query-entity pairs is $f_{qe_1}(p)=f_{qe_2}(p)=2$ because the passage has two entities in common with the list retrieved for $q$.

\subsubsection{Retrieving passages using combined query and entity} 
\label{subsubsec: baseline 2}
We retrieved passages using a compound query, where the query is a combination of the original query and the entity for which we are trying to retrieve support passage.
 
\subsection{Results}
\label{subsubsec: reults}
\todo{Maybe say some general words or merge with next paragraph. Maybe say someting about the baselines}

\todo{why is Entity Context Neighbors MAP, P@R, MRR higher than Retrieval score of a pseudo-document --- but its P@1 score is twice as high as Entity COntext Neighbors?!? Is this a copy/paste error? P@1 is a stricter version of MRR, which is a precision oriented version of MAP, they may differ slightly but will not deviate by this much. (or did you forget -c in trec\_eval? }

\begin{table*}
  \caption{Performance of entity support passage ranking methods. \todo{include standard errors e.g. $0.0946\pm0.01$ or paired t-tests,state $\alpha=5\%$ and give $0.0946^\triangle$ if significantly improved $0.0946^\triangledown$ for signficantly worse}}
  \label{tab:results}
  \scalebox{0.9}{
  \begin{tabular}{lp{2cm}p{2cm}p{2cm}p{2cm}l}
    \toprule
    & MAP & P@R & MRR & P@1 \\
    \midrule
    Baseline 1&	0.0946$\pm$0.0023&	0.059$\pm$0.0023&	0.1118$\pm$0.0026&	
    0.0613$\pm$0.0025 \\
Baseline 2&	0.0672$\pm$0.0026&	0.0572$\pm$0.0026&	0.082$\pm$0.0029&	
0.0624$\pm$0.0028 \\
\midrule
    Entity Context Neighbors & \textbf{0.3136$\pm$0.0038} & 0.2793$\pm$0.004 & 0.3590$\pm$0.0039 & 0.2977$\pm$0.0044 \\
    
    Retrieval score of a pseudo-document	& 0.1281$\pm$0.0036&	0.1106$\pm$0.004 &	0.1491$\pm$0.0037&	0.1599$\pm$0.0044 \\
    \midrule
    
    QE with words from pseudo-document (BM25+RM1)\textsuperscript{*}& 0.0959$\pm$0.0034&	0.0785$\pm$0.0036&	0.1189$\pm$0.0036&	
    0.1278$\pm$0.0044 \\
    
QE with words from pseudo-document (LM-DS + RM1)\textsuperscript{*}&	0.0932$\pm$0.0032&	0.0736$\pm$0.0034&	0.1164$\pm$0.0035&	
0.125$\pm$0.0044 \\

QE with words from pseudo-document (LM-JM + RM1)\textsuperscript{*}&	0.1212$\pm$0.0036&	0.1033$\pm$0.0039&	0.1429$\pm$0.0037&	
0.1533$\pm$0.0044 \\

QE with words from pseudo-document (BM25+RM3)\textsuperscript{*}&	0.1000$\pm$0.0033&	0.0814$\pm$0.0035&	0.1243$\pm$0.00035&	
0.1333$\pm$0.0044 \\

QE with words from pseudo-document (LM-DS+RM3)\textsuperscript{*}&	0.0935$\pm$0.0032&	0.0739$\pm$0.0034&	0.1167$\pm$0.0035&	
0.1256$\pm$0.0044 \\

QE with words from pseudo-document (LM-JM+RM3)\textsuperscript{*}&	\textbf{0.1231$\pm$0.0036}&	0.1056$\pm$0.0039&	0.1448$\pm$0.0037&	
0.1546$\pm$0.0044 \\
\midrule

QE with entities from pseudo-document (BM25+RM1)\textsuperscript{*}&	0.2358$\pm$0.0036&	0.1978$\pm$0.0038&	0.2725$\pm$0.0038&	
0.2135$\pm$0.0041 \\

QE with entities from pseudo-document (LM-DS+RM1)\textsuperscript{*}&	0.2573$\pm$0.0037&	0.2200$\pm$0.0039&	0.2967$\pm$0.0039&	
0.2363$\pm$0.0042 \\

QE with entities from pseudo-document (LM-JM+RM1)\textsuperscript{*}&	0.2736$\pm$0.0037&	0.2346$\pm$0.0039&	0.3148$\pm$0.0039&	
0.2535$\pm$0.0043 \\

QE with entities from pseudo-document (BM25+RM3)\textsuperscript{*}&	0.2738$\pm$0.0037&	0.2345$\pm$0.0039&	0.3129$\pm$0.0039&	
0.2521$\pm$0.0043 \\

QE with entities from pseudo-document (LM-DS+RM3)\textsuperscript{*}&	0.2854$\pm$0.0037&	0.2479$\pm$0.004&	0.3256$\pm$0.0039&	
0.2653$\pm$0.0043 \\

QE with entities from pseudo-document (LM-JM+RM3)\textsuperscript{*}&	\textbf{0.29980$\pm$.0038}&	0.2611$\pm$0.004&	0.3414$\pm$0.0039&	
0.2801$\pm$0.0044 \\

Passage filtering with entities\textsuperscript{*} &	0.2736$\pm$0.0037&	0.2321$\pm$0.0039&	0.3085$\pm$0.0038&	
0.2376$\pm$0.0042 \\

Salience score of passage containing frequently co-occurring entities\textsuperscript{*}&	0.0173$\pm$0.0025&	0.0177$\pm$0.0026&	0.0303$\pm$0.003&	
0.0243$\pm$0.0035 \\

Salience score of passage in pseudo-document\textsuperscript{*}&	0.0231$\pm$0.0028&	0.0234$\pm$0.0028&	0.0365$\pm$0.0036&	
0.0314$\pm$0.0036 \\
\midrule
L2R-5-fold-CV with all features & 0.2837$\pm$0.0039&	0.2571$\pm$0.004&	0.3278$\pm$0.0041&	
0.2869$\pm$0.0042 \\

\midrule
L2R-5-fold-CV with features marked $*$ & \textbf{0.3214$\pm$0.0037}&	0.2781$\pm$0.004&	0.3707$\pm$0.0039&	
0.3034$\pm$0.004 \\

			
  \bottomrule
\end{tabular}}
\end{table*}

The results from our method are shown in Table \ref{tab:results}. Below, we discuss each of the research questions presented in Section \ref{subsec:research questions}.

\textbf{RQ1} We observe in Table \ref{tab:results} that ranking passages with contextual entities is the best performing method overall with a MAP of 0.313. This shows that considering other entities in context helps the retrieval of support passages. Moreover, we also observe that this method achieves significant improvement over the two baselines, which have a MAP of 0.094 and 0.067 respectively. From Table \ref{tab:results} we observe that performance of the learning-to-rank combination of all features reduces slightly as compared to this method which achieves the best MAP score. To investigate further, we tried a learning-to-rank combination of different subsets of features and found that the subset which does not contain the features \textit{entity context neighbors} and \textit{retrieval score of pseudo-document} performs the best in terms of MAP. We report the result in Table \ref{tab:results}. We can see that although the feature \textit{entity context neighbors} works very well on its own, it does not mix so well with the other features. This happens because the feature sometimes works very well and sometimes does not which confuses learning-to-rank. 

\textbf{RQ2} We observe in Table \ref{tab:results} that the two methods which rank passages using entity salience perform the worst. This is the same case when we mix them with other features using learning-to-rank.\todo{Sure, but how are they performing when combined with your other joint features?} We investigated this further and found that entities marked by SWAT as salient are indeed salient and those marked non-salient are indeed non-salient. However, more than 50\% of the entities retrieved for the query (by the external system) are non-salient in the passages mentioning them. Hence, we need better candidate pool because BM25, Query Likelihood, Relevance Model did not include the entity during retrieval , and do not have any notion of salience. \todo{No, you can't blame it on the entity ranking method if your candidate method is not able to find good support passages! I would rephrase saying "we need better candidate pool because BM25, QL, RM did not include the entity during retrieval , and dont have any notion of salience} 

\textbf{RQ3} We observe that among all methods which retrieve with contextual entities, the method using LM-JM with RM3 performs the best with a MAP of 0.299. Note that here we use RM3 to expand the query with entities (see Section \ref{subsubsec:features:1}(\ref{en: Query expansion using entities from pseudo-documents})). Also, among all methods which retrieve with contextual words, the methods using LM-JM with RM3 performs the best with a MAP of 0.123. \todo{Is your salience feature working better on some of these rankings (say with entity-RM) than without (say BM25?) }  As we can see, retrieval with contextual entities outperforms retrieval with contextual words and hence shows that contextual entities are indeed more informative than contextual words. \todo{make clear above that your RM3 is expanding with entities .. usually people use RM3 to expand with words.}  Another observation is that among BM25, LM-DS and LM-JM, retrieval with LM-JM always gives the best performance for both contextual entities and contextual words. 

\section{Conclusion and Future Work}
\label{sec: conclusion}
This work addresses the task of support passage retrieval to enrich entity rankings in response to a search query. We propose joint query-entity-passage ranking methods and present some initial results. In particular, we show that co-occurring entities are an important indicator of which passages might support an entity for a query and that retrieving passages using a compound query made of the original query and the entity is not enough to solve the problem. We also experiment with entity salience and study its effect on the task. We identify a need to develop new indexing and retrieval methods that integrate entity salience in an early phase, since for many target entities, no salient passages were available in the candidate set.
%As future work, it would be interesting to see how incorporating salience while constructing the entity pseudo-documents affects the performance and how expanding with salient entities performs.

%
% The next two lines define the bibliography style to be used, and the bibliography file.
%\begin{acks}
%We would like to thank everyone who gave useful insights to improve the paper and helped to make it more readable. In particular, we would like to thank Jordan Ramsdell from the TREMA lab at UNH who gave many useful insights into the problem and helped to polish it further. 
%\end{acks}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\end{document}
