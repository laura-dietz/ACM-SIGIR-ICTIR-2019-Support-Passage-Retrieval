% The first command in your LaTeX source must be the \documentclass command.

\documentclass[sigconf]{acmart}
 % Do not change for ICTIR'19

\settopmatter{printacmref=true}
  % mandatory for ICTIR'19

\fancyhead{}
  % do not delete this code.
\usepackage{balance}
  % for creating a balanced last page (usually last page with references)
 %activate todo's
%\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
% deactivate todos
\newcommand{\todo}[1]{ \PackageWarning{TODO:}{#1!}}

% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.

\copyrightyear{2019} 
\acmYear{2019} 
\acmConference[ICTIR '19]{The 2019 ACM SIGIR International Conference on the Theory of Information Retrieval}{October 2--5, 2019}{Santa Clara, CA, USA}
\acmBooktitle{The 2019 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR '19), October 2--5, 2019, Santa Clara, CA, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3341981.3344243}
\acmISBN{978-1-4503-6881-0/19/10}


% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
\acmSubmissionID{ictir094s}


% end of the preamble, start of the body of the document source.

\begin{document}

\fancyhead{}
  % do not delete this code.


% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title[Support Passage Retrieval]{Why does this Entity matter? Support Passage Retrieval for Entity Retrieval}


% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Shubham Chatterjee}
\email{sc1242@cs.unh.edu}
%\orcid{1234-5678-9012}
\affiliation{
 \institution{University of New Hampshire}
  \city{Durham}
 \state{New Hampshire}
}
\author{Laura Dietz}
\email{dietz@cs.unh.edu}
\affiliation{
 \institution{University of New Hampshire}
  \city{Durham}
 \state{New Hampshire}
}

%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Chatterjee and Dietz}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
Our goal is to complement an entity ranking with human-readable explanations of how those retrieved entities are connected to the information need. In this paper, we present a formalization of the problem and present some methods which may be employed to solve this problem. In particular, we develop some joint query-entity and passage features to rank passages for query-entity pairs. We also study the effect of entity salience in this task. The effectiveness of the indicators are studied within a supervised learning-to-rank framework. 
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
<concept_id>10002951.10003317.10003338</concept_id>
<concept_desc>Information systems~Retrieval models and ranking</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
<concept>
<concept_id>10002951.10003317.10003325</concept_id>
<concept_desc>Information systems~Information retrieval query processing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
\end{CCSXML}

\ccsdesc[500]{Information systems~Retrieval models and ranking}
\ccsdesc[300]{Information systems~Information retrieval query processing}


%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{joint query-entity-passage features, entity context neighbors, entity salience, entity context document}


%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:Introduction}

Entity ranking is important for applications which seek either a particular entity or a list of entities, for example, entities involved in the \textit{Brexit} such as \texit{Theresa May}.  Several studies show that 40-70\% of all web searches target entities \cite{guo2009named,balog2018entity}. Such queries are best answered by giving the user a ranking of relevant entities. However, in addition to the ranking of entities, presenting a short passage to the user which tells them how or why the entity is related to the query may be more helpful since it helps users decide if they want to know more about the entity. Analogous to how most modern search engines display snippets for the query to the user in case of document retrieval, we want to present a short passage about the entity that explains to the user why the entity is relevant to the query without conducting further research. We study the following task:

\textbf{Task:} Given a user's information need $Q$; an external system predicts a ranking of relevant entities $E$. Our task is to, for every relevant entity $e_i \in E$, retrieve and rank $K$ passages $s_{ik}$ that explain why this entity $e_i$ is relevant for $Q$. 


% (you already said this; I used this instead of example above) For example, if we know that the entity \textit{Theresa May} is relevant to the query \textit{brexit}, we want a passage that is not only relevant and mentions \textit{Theresa May} in the context of the query but also that the entity is central to the passage, that is, salient in the passage. 
In addition to being relevant for $Q$, those passages $s_{ik}$ should mention the entity $e_i$ in a salient way. By salient, we mean that the entity is central to the passage. (See Section \ref{sec: entity salience}).
In this paper, we look at the problem of complementing an entity ranking with descriptions of why the entity is relevant to the query and present some first results. This task is useful whenever entities are displayed along with web search results, such as entity cards \cite{berntson2012providing}. The novel contribution of this work is that we consider a joint model to rank passages for a given query and entity and compare it to two baselines, one which is a state-of-the-art which retrieves passages for the query-entity pair using a compound query (made of the query and entity terms) and another which scores passages for the query-entity pair by the number of relevant entities (for the query) in the passage. We also present features which consider the salience of the entity in the passage and use them to retrieve passages. We then study the effectiveness of these methods in a learning-to-rank \cite{liu2009learning} setting.

As an example, consider the following query-entity pair and the corresponding support passage : \\
\textbf{Query: } Unfree labour \\
\textbf{Entity: } Detention (imprisonment) \\
\textbf{Support Passage: } \\
Unfree labour is a generic or collective term for those work relations, especially in modern or early modern history, in which people are employed against their will by the threat of destitution, detention, violence (including death), lawful compulsion, or other extreme hardship to themselves or to members of their families.

Our hypothesis is that incorporating entity salience should help; in this work we study if we have the necessary tools to reap benefits from entity salience. The issues our study reveals are not due to failures of the salience detection model, but because the retrieval models available today are not modelling salience, and therefore passages in the pool do not mention the entity in salient ways. 



\section{Related Work}
\label{sec:related work}
%\paragraph{\textbf{Sentence Retrieval}}
Sentence Retrieval retrieves relevant sentences in response to a query, question or another sentence. Tasks such as question answering \cite{cardie2000examining}, summarization, novelty detection, topic detection and tracking \cite{stokes2001first}, and information provenance make use of a sentence retrieval module as a pre-processing step. However, none of these address the task of support sentence retrieval. The closest is perhaps the work by Cardie et al.\ \cite{cardie2000examining} which discusses sentence ranking models where the query includes a constraint on a type of entity(e.g. a location, a person). 
Blanco and Zaragoza \cite{blanco2010finding} present a model that ranks entity support sentences with learning-to-rank. Their work focuses on features based on named entity recognition (NER) in combination with term-based retrieval models. 
%\paragraph{\textbf{Entity Relation Explanation}}

Voskarides et al.\ \cite{voskarides2015learning} study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. They model the task as a learning-to-rank problem with a rich set of features which include textual, entity and relationship features. Aggarwal et al.\ \cite{aggarwal2016connecting} rank all the paths between any two entities in a knowledge graph. This can help in explaining relationships between seemingly unconnected entities. 

%\paragraph{\textbf{Support Passage Retrieval}}
Kadry and Dietz \cite{kadry2017open} use relation extraction using OpenIE for support passage retrieval. Their work studies how relation extraction can help in support passage retrieval and what are the limitations of the current relation extraction approaches that need to be overcome. It describes a rich set of features which can be used for the task.  

None of the works above study joint query-entity and paragraph relevance nor do they consider the effect of entity salience. We propose to look at features that model the joint relevance of a paragraph to an entity and a query, which captures the entity salience in the passage. 

\section{Background: Entity Salience}
\label{sec: entity salience}
Consider the query \textit{brexit} and the entity \textit{Theresa May} retrieved for the query. Now consider the following two passages: \\
\textbf{Passage 1: }  If Labour and the Conservatives fail to reach an agreement, MPs will face a series of votes on Brexit options, which could include another referendum. Theresa May has said her government stands ready to abide by the decision of the House  if Labour does the same.\\
\textbf{Passage 2: } British Prime Minister Theresa May offered a new Brexit plan on Tuesday, in a last-ditch effort to get her still-unpopular Brexit deal approved.

Although both passages mention \textit{Theresa May} and is about \textit{brexit}, Passage 2 talks about how Theresa May is affecting brexit and hence the entity is central to the passage whereas in Passage 1 it is not.

%We notice that both passages mention the entity Theresa May and is about the query \textit{brexit}.However, in Passage 1 Theresa May is not central, whereas in Passage 2 it is (the passage talks about how Theresa May is affecting brexit).  This is an example of a good support passage, one which not only mentions the entity and makes a connection between the query-entity pair, but also mentions the entity in a salient, that is, central way. 
%\todo{since you have some space, I would explain what entity salience means or does not mean. It is likely that the reviewer does not know it. (yes, shocking!) SO if you spend 1 paragraph, maybe one picture with two examples following the brexit / Theresa may will be good.}


\section{Approach}
\label{sec:approach}
%  not needed: \subsection{Overview}
%\label{subsec:overview}
Given a ranked list of entities for a query (by an external system which predicts entities), we seek to embellish this ranking with passages which would explain to the user why the entity is relevant to the query. To model this joint relevance for \textit{both} the query and the entity, we use the co-occurring entities with a given entity. 

\subsection{Candidate Passage Generation}
\label{subsec:candidate passage generation}
We generate the candidate set of passages using a learning-to-rank combination of the following retrieval models. The RM1/RM3 are expanding with words in this case. Later, we also use RM1/RM3 for expanding the query with entities. 
%\todo{Make clear that your RM3/RM1 is expanding with words. Later you are expanding with entities (expanding wth entities is something I like to do, but its not usual in the community}:
\begin{itemize}
    \item BM25 (default Lucene) with RM1 (BM25 + RM1)
    \item BM25 (default Lucene) with RM3 (BM25 + RM3)
    \item Language Models with Jelinek-Mercer smoothing $(\lambda = 0.4)$, with RM1 (LM-JM + RM1)
    \item Language Models with Jelinek-Mercer smoothing $(\lambda = 0.4)$, with RM3 (LM-JM + RM3)
    \item Language Models with Dirichlet smoothing, with RM1 (LM-DS + RM1)
    \item Language Models with Dirichlet smoothing, with RM3 (LM-DS + RM3)
\end{itemize}


\subsection{Features}
\label{subsec:features}
\subsubsection{Features based on co-occurring entities}
\label{subsubsec:features:1}
We assume that a passage with a lot of query-relevant entities which co-occur frequently with a given entity is likely talking about that entity. To find most frequently co-occurring entities with a given entity, we filter all passages (retrieved for the query) mentioning the entity and "stitch" them together into $D_e$ which we call an \textit{Entity Context Document} (ECD) about the entity $e$, following an idea of Dalton et al.\ \cite{dalton2014entity} %(Figure \ref{fig:pseudo-doc}). 
All entities $e' \in D_e$ co-occur with $e$. A distribution over these co-occurring entities would tell us how likely we are to see the co-occurring entity $e'$ provided we have already seen the query-entity pair $(q, e)$. We derive this distribution by finding the number of times an entity $e'$ occurs in the ECD. More formally, 

\begin{equation}
\label{eq:1}
    P(e'|e,q) \propto \sum_{p \in D_e} count(e'\in p)
\end{equation}
where $p$ is a paragraph, $D_e$ is the ECD about entity $e$, $e' (\ne e)$ is an entity co-occurring with $e$, and $count(e')$ is the number of $e'$ in $p$. 

\begin{enumerate}
    \item \textbf{Entities in the Entity Context Neighbors (ECN):} \label{en:entity context neighbors} We score a passage in an ECD $D_e$ by accumulating the co-occurrence scores of entities $e'$ contained therein. 
More formally, we can define a feature value $f_{qe}(p)$ of a paragraph $p \in D_e$ for query-entity pair $(q, e)$ as
\begin{equation}
\label{eq:2}
    f_{qe}(p) = \sum_{e' \in p} P(e'|e,q)
\end{equation}
%\begin{figure}
 %\centering 
 %\includegraphics[scale=0.3]{ecd.PNG}
 %\caption{Entity Context Document}
 %\label{fig:pseudo-doc}
%\end{figure}
\item \textbf{Query expansion using entities from entity context documents:} \label{en: Query expansion using entities from entity context documents} Using top 20 co-occurring entities to expand the query using RM1 and RM3 on a search index that has a separate field for entities. We experiment with the same variations as given in Section \ref{subsec:candidate passage generation}. Each variation becomes a feature.
\end{enumerate}
\subsubsection{Features based on entity context documents}
\label{subsubsec:feature:2}
\begin{enumerate}
    \item \textbf{Retrieval score of an entity context document} \label{en:Retrieval score of a entity context document}For every query, we retrieve ECDs using BM25. There is one ECD per entity. A passage might mention several entities and hence may be present in the ECD of more than one entity. 
    As in Section \ref{subsubsec:features:1} (\ref{en:entity context neighbors}), we obtain a passage score by accumulating the scores of the ECD it appears in. 

    \item \textbf{Query expansion using words from entity context document} \label{en:Query expansion using words from entity context document} Similar to Section \ref{subsubsec:features:1} (\ref{en: Query expansion using entities from entity context documents}), we expand the query using terms from the ECDs using pseudo relevance feedback. We use top 50 terms for expansion and 100 documents as the feedback set. We experiment with the same variations as given in Section \ref{subsec:candidate passage generation}. Each variation becomes a feature.
\end{enumerate}

\subsubsection{Features based on entity salience}
\label{subsubsec:feature:3}
We use SWAT \cite{ponza2018swat} to annotate passages with salient entities and salience score and class (whether salient or not). We score a passage $p$ for the query-entity pair $(q,e)$ as
\begin{equation}
\label{eq:3}
    Score(p | e, q) = Score(p | q) * Score(e | p)
\end{equation}
 where $Score(p | q)$ is the normalized retrieval score of $p$ (obtained from the passage ranking) and $Score(e | p)$ is the normalized salience score of $e$ for $p$. We use Equation \ref{eq:3} in two ways:
\begin{enumerate}
    \item \textbf{Salience score of passage with co-occurring entities}.
    Re-rank passages obtained using method in Section \ref{subsubsec:features:1}(\ref{en:entity context neighbors}).
    
    \item \textbf{Salience score of passage in a entity context document} Score passages in an ECD about an entity. 
\end{enumerate}
\section{Evaluation}
\label{sec:Evaluation}
\subsection{Research Questions}
\label{subsec:research questions}
Our goal is to study the following research questions.\\
\textbf{RQ1} To what extent do contextual entities affect the support passage retrieval?  \\
\textbf{RQ2} To what extent does entity salience affect the support passage retrieval?  \\
\textbf{RQ3} Are contextual entities more informative than contextual words? 

\subsection{Evaluation Paradigm}
\label{subsec:Evaluation Paradigm}
We use the dataset from the TREC Complex Answer Retrieval track \cite{dietztrec}\footnote{\url{http://trec-car.cs.unh.edu}} to evaluate our methods. It contains both passage and entity ground truth data as well as an entity linked corpus consisting of Wikipedia passages for the entire Wikipedia. We use the entity links provided in the collection as well as those annotated using TagMe \cite{ferragina2010tagme}. We derive a ground truth for entity support passage retrieval from the ground truth of relevant passages and entities provided with the data set (article-level) as follows: any true passage that contains an entity link to the query entity is defined as relevant for the given query and entity. We apply our methods to produce a passage ranking for every query-entity pair and do 5-fold cross validation. We use RankLib \footnote{Dang, V. "The Lemur Project-Wiki-RankLib." Lemur Project,[Online]. Available: \url{http://sourceforge. net/p/lemur/wiki/RankLib}.} with Coordinate Ascent
optimization to optimize for Mean Average Precision(MAP). We use MAP, Mean Reciprocal Rank (MRR), Precision at R (P@R) and Precision at 1 (P@1) as our evaluation metrics.   
\subsection{Baseline}
\label{subsec:Baseline}
\subsubsection{Rank score proportional to the frequency of entity links to an entity} 
\label{subsubsec: baseline 1}
We rank passages for a query-entity pair by the number of relevant entities in the passage. For example, if a passage $p$ contains entities $\{e_1,e_2\}$ and the entities $\{e_1,e_2,e_3,e_4\}$ have been retrieved for the query $q$, then the score of $p$ for each of the query-entity pairs is $f_{qe_1}(p)=f_{qe_2}(p)=2$ because the passage has two entities in common with the list retrieved for $q$.

\subsubsection{Retrieving passages using combined query and entity} 
\label{subsubsec: baseline 2}
We retrieved passages using a compound query, where the query is a combination of the original query and the entity for which we are trying to retrieve support passage.
 
\subsection{Results}
\label{subsubsec: reults}
\begin{table}
  \caption{Performance of entity support passage ranking methods \todo{include standard errors e.g. $0.0946\pm0.01$ or paired t-tests,state $\alpha=5\%$ and give $0.0946^\triangle$ if significantly improved $0.0946^\triangledown$ for signficantly worse}}
  \label{tab:results}
  \scalebox{0.6}{
  \begin{tabular}{lp{2cm}p{2cm}p{2cm}p{2cm}l}
    \toprule
    & MAP & P@R & MRR & P@1 \\
    \midrule
    Baseline 1&	0.0946$\pm$0.0023&	0.059$\pm$0.0023&	0.1118$\pm$0.0026&	
    0.0613$\pm$0.0025 \\
Baseline 2&	0.0672$\pm$0.0026&	0.0572$\pm$0.0026&	0.082$\pm$0.0029&	
0.0624$\pm$0.0028 \\
\midrule
    Entity Context Neighbors & \textbf{0.3136$\pm$0.0038} & \textbf{0.2793$\pm$0.004}& \textbf{0.3590$\pm$0.0039} &
\textbf{0.2977$\pm$0.0044} \\
    
    Retrieval score of an ECD	& 0.1281$\pm$0.0036&	0.1106$\pm$0.004 &	0.1491$\pm$0.0037&	0.1599$\pm$0.0044 \\
    \midrule
    QE with words (BM25+RM1)& 0.0959$\pm$0.0034&	0.0785$\pm$0.0036&	0.1189$\pm$0.0036&	
    0.1278$\pm$0.0044 \\
    
QE with words (LM-DS + RM1)&	0.0932$\pm$0.0032&	0.0736$\pm$0.0034&	0.1164$\pm$0.0035&	
0.125$\pm$0.0044 \\

QE with words  (LM-JM + RM1)&	0.1212$\pm$0.0036&	0.1033$\pm$0.0039&	0.1429$\pm$0.0037&	
0.1533$\pm$0.0044 \\

QE with words (BM25+RM3)&	0.1000$\pm$0.0033&	0.0814$\pm$0.0035&	0.1243$\pm$0.00035&	
0.1333$\pm$0.0044 \\

QE with words  (LM-DS+RM3)&	0.0935$\pm$0.0032&	0.0739$\pm$0.0034&	0.1167$\pm$0.0035&	
0.1256$\pm$0.0044 \\

QE with words (LM-JM+RM3)&	\textbf{0.1231$\pm$0.0036}&	\textbf{0.1056$\pm$0.0039}&	\textbf{0.1448$\pm$0.0037}&	
\textbf{0.1546$\pm$0.0044} \\
\midrule

QE with entities (BM25+RM1)&	0.2358$\pm$0.0036&	0.1978$\pm$0.0038&	0.2725$\pm$0.0038&	
0.2135$\pm$0.0041 \\

QE with entities (LM-DS+RM1)&	0.2573$\pm$0.0037&	0.2200$\pm$0.0039&	0.2967$\pm$0.0039&	
0.2363$\pm$0.0042 \\

QE with entities (LM-JM+RM1)&	0.2736$\pm$0.0037&	0.2346$\pm$0.0039&	0.3148$\pm$0.0039&	
0.2535$\pm$0.0043 \\

QE with entities (BM25+RM3)&	0.2738$\pm$0.0037&	0.2345$\pm$0.0039&	0.3129$\pm$0.0039&	
0.2521$\pm$0.0043 \\

QE with entities  (LM-DS+RM3)&	0.2854$\pm$0.0037&	0.2479$\pm$0.004&	0.3256$\pm$0.0039&	
0.2653$\pm$0.0043 \\

QE with entities  (LM-JM+RM3)&	\textbf{0.29980$\pm$.0038}&	\textbf{0.2611$\pm$0.004}&	\textbf{0.3414$\pm$0.0039}&	
\textbf{0.2801$\pm$0.0044} \\
\midrule

Passage filtering with entities &	0.2736$\pm
$0.0037&	0.2321$\pm$0.0039&	0.3085$\pm$0.0038&	
0.2376$\pm$0.0042 \\

Salience score of passage containing \\ frequently co-occurring entities&	0.0173$\pm$0.0025&	0.0177$\pm$0.0026&	0.0303$\pm$0.003&	
0.0243$\pm$0.0035 \\

Salience score of passage in ECD&	0.0231$\pm$0.0028&	0.0234$\pm$0.0028&	0.0365$\pm$0.0036&	
0.0314$\pm$0.0036 \\
\midrule
L2R-5-fold-CV with all features & 0.2813$\pm$0.0039&	0.2402$\pm$0.004&	0.3193$\pm$0.0041&	
0.2869$\pm$0.0042 \\


			
  \bottomrule
\end{tabular}}
\end{table}

The results from our method are shown in Table \ref{tab:results}. Below, we discuss each of the research questions presented in Section \ref{subsec:research questions}.

\textbf{RQ1} We observe in Table \ref{tab:results} that ranking passages with contextual entities is the best performing method overall with a MAP of 0.313. This shows that considering other entities in context helps the retrieval of support passages. Moreover, we also observe that this method achieves significant improvement over the two baselines, which have a MAP of 0.094 and 0.067 respectively. We discuss more about this in Section \ref{subsec: ablation_study}.

\textbf{RQ2} We observe in Table \ref{tab:results} that the two methods which rank passages using entity salience perform the worst. This is the same case when we mix them with other features using learning-to-rank. On further investigation we find that SWAT correctly identifies salient and non-salient entities.  However, while most of the entities that have a salient passage (in the candidate pool) are also relevant (according to the entity ground truth), a majority (95\%) of relevant entities (in the ground truth) do not have a passage with a salient mention in the candidate pool. Therefore the salience feature is only available to few entities and therefore only has a limited impact on the overall result. Hence, we need better candidate pool because BM25, Query Likelihood, Relevance Model did not include the entity during retrieval , and do not have any notion of salience.

To study whether salience is a useful indicator when it is available, we analyze results on the subset of rankings for query-entity pairs for which the passage ranking contains at least one passage in which the query-entity is salient. The results are shown in Table \ref{tab:salience results}. We observe that the salience indicator performs very well as compared to other methods. Hence, salience is a useful indicator but only for entities which have a salient passage in the candidate pool. We only include the two baselines and the top performing method from earlier here for comparison. Refer to the online appendix for more results.  
\begin{table}
  \caption{Analysis of Salience \todo{include standard errors e.g. $0.0946\pm0.01$ or paired t-tests,state $\alpha=5\%$ and give $0.0946^\triangle$ if significantly improved $0.0946^\triangledown$ for signficantly worse}}
  \label{tab:salience results}
  \scalebox{0.6}{
  \begin{tabular}{lp{2cm}p{2cm}p{2cm}p{2cm}l}
    \toprule
    & MAP & P@R & MRR & P@1 \\
    \midrule
    Baseline 1&	0.1568$\pm$0.0111&	0.1083$\pm$0.0109&	0.2140$\pm$0.0130&	
    0.1192$\pm$0.0132 \\
Baseline 2&	0.0913$\pm$0.0106&	0.0814$\pm$0.0106&	0.1451$\pm$0.0139&	
0.0993$\pm$0.0137 \\
\midrule
Salience score of passage containing \\ frequently co-occurring entities&	
\textbf{0.4722$\pm$0.0161}&	\textbf{0.4779$\pm$0.0160}&	\textbf{0.7451$\pm$0.0146}&	
\textbf{0.6423$\pm$0.0195} \\

Salience score of passage in ECD&	0.2897$\pm$0.0154&	0.3006$\pm$0.0152&	0.497$\pm$0.0160&	
0.4023$\pm$0.0203 \\

Entity Context Neighbors & 0.4067$\pm$0.0151 & 0.3407$\pm$0.0163& 0.5252$\pm$0.0156 &
0.3741$\pm$0.0196 \\
\bottomrule
\end{tabular}}
\end{table}

\textbf{RQ3} We observe that among all methods which retrieve with contextual entities, the method using LM-JM with RM3 performs the best with a MAP of 0.299. Note that here we use RM3 to expand the query with entities (see Section \ref{subsubsec:features:1}(\ref{en: Query expansion using entities from entity context documents})). Also, among all methods which retrieve with contextual words, the methods using LM-JM with RM3 performs the best with a MAP of 0.123.  As we can see, retrieval with contextual entities outperforms retrieval with contextual words and hence shows that contextual entities are indeed more informative than contextual words. Another observation is that among BM25, LM-DS and LM-JM, retrieval with LM-JM always gives the best performance for both contextual entities and contextual words. We discuss more about this in Section \ref{subsec: ablation_study}.

\subsection{Subset Ablation Study}
\label{subsec: ablation_study}
From Table \ref{tab:results} we observe that performance of the learning-to-rank combination of all features reduces slightly as compared to method \textit{Entity Context Neighbors} which achieves the best MAP score. To investigate this and to find which subsets of features were more important, we did a subset ablation study where we combined different subsets of features using learning-to-rank with 5-fold cross-validation. 

We tried a learning-to-rank combination of the following subsets of features:
\begin{enumerate}
    \item \textbf{Subset-1}: All features except the QE with word features.
    \item \textbf{Subset-2}: All features except the QE with entities features.
     \item \textbf{Subset-3}: All features except \textit{entity context neighbors} and \textit{retrieval score of entity context document}
    \item \textbf{Subset-4}: Only the QE with word features.
    \item \textbf{Subset-5}: Only the QE with entity features.
\end{enumerate}
The results are shown in Table \ref{tab:ablation_study}.
\begin{table}
  \caption{Subset Ablation Study}
  \label{tab:ablation_study}
  \scalebox{0.75}{
  \begin{tabular}{{lp{2cm}p{2cm}p{2cm}p{2cm}l}}
    \toprule
    & MAP & P@R & MRR & P@1 \\
    \midrule
    Subset-1&	
    0.2801$\pm$0.0037& 
    0.2405$\pm$00.0039&
    0.3163$\pm$0.0039&
    0.4403$\pm$0.0044&
    \\	
    Subset-2&	
    0.3467$\pm$0.0038&
    0.3222$\pm$0.004&
    0.04056$\pm$0.004&
    0.5217$\pm$0.004&
    \\
    Subset-3&	
    0.2789$\pm$0.0037&
    0.2388$\pm$0.0039&
    0.3160$\pm$0.0038&
    0.4320$\pm$0.0044&
    \\	
    Subset-4&	
    0.1597$\pm$0.0036&
    0.1346$\pm$0.0038&
    0.1896$\pm$0.0037&
    0.36670$\pm$0.0043&
    \\
    Subset-5&	
    0.3047$\pm$0.0038&
    0.2666$\pm$0.004&
    0.3465$\pm$0.0039&
    0.4213$\pm$0.0044&
    \\
 \bottomrule
\end{tabular}}
\end{table}
We observe that the subset-2 with all features except the QE with entity features achieves a significant improvement over the learning-to-rank combination of all features. However, a learning-to-rank combination of only these QE with entity features (subset-5) performs well. This shows that the subset-5 features perform well on their own but do not mix well with all the features. Moreover, the QE with word features (subset-4) do not perform well on their own but mix well with other features to boost the MAP score. From the results we can see that contextual entities affect support passage retrieval to a greater extent as compared to contextual words. (RQ1)

We also observed that the weights assigned by Coordinate Ascent to the features in subset-4 are equal whereas those assigned to features in subset-5 are not. This shows that all the subset-4 features give the same information and contextual entities are indeed more informative than contextual words. (RQ3)
\section{Conclusion and Future Work}
\label{sec: conclusion}
This work addresses the task of support passage retrieval to enrich entity rankings in response to a search query. We propose joint query-entity-passage ranking methods and present some initial results. In particular, we show that co-occurring entities are an important indicator of which passages might support an entity for a query and that retrieving passages using a compound query made of the original query and the entity is not enough to solve the problem. We also experiment with entity salience and study its effect on the task. We identify a need to develop new indexing and retrieval methods that integrate entity salience in an early phase, since for many target entities, no salient passages were available in the candidate set.
%As future work, it would be interesting to see how incorporating salience while constructing the entity entity context documents affects the performance and how expanding with salient entities performs.

%
% The next two lines define the bibliography style to be used, and the bibliography file.
%\begin{acks}
%We would like to thank everyone who gave useful insights to improve the paper and helped to make it more readable. In particular, we would like to thank Jordan Ramsdell from the TREMA lab at UNH who gave many useful insights into the problem and helped to polish it further. We would also like to thank the Graduate School at the University of New Hampshire for providing a travel grant to attend the conference.
%\end{acks}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
% If your work has an appendix, this is the place to put it.
\appendix

\section{Online Resources}
For more experiments, results, descriptions of methods and datasets, refer to the online appendix for this paper at:  \url{https://shubham526.github.io/support-passage-retrieval/.}
\end{document}
